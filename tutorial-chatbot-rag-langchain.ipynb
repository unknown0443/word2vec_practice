{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q langchain langchain-community langchain_huggingface chromadb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:51:16.895228Z","iopub.execute_input":"2025-01-23T07:51:16.895648Z","iopub.status.idle":"2025-01-23T07:51:29.147466Z","shell.execute_reply.started":"2025-01-23T07:51:16.895609Z","shell.execute_reply":"2025-01-23T07:51:29.145894Z"}},"outputs":[],"execution_count":101},{"cell_type":"markdown","source":"## 1. Use langchain RAG","metadata":{}},{"cell_type":"code","source":"import os\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:51:31.829685Z","iopub.execute_input":"2025-01-23T07:51:31.830156Z","iopub.status.idle":"2025-01-23T07:51:31.836667Z","shell.execute_reply.started":"2025-01-23T07:51:31.830114Z","shell.execute_reply":"2025-01-23T07:51:31.835275Z"}},"outputs":[],"execution_count":102},{"cell_type":"code","source":"os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"u r api\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T06:21:34.627192Z","iopub.execute_input":"2025-01-23T06:21:34.627751Z","iopub.status.idle":"2025-01-23T06:21:34.632914Z","shell.execute_reply.started":"2025-01-23T06:21:34.627705Z","shell.execute_reply":"2025-01-23T06:21:34.631806Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:32:24.841782Z","iopub.execute_input":"2024-11-06T03:32:24.842623Z","iopub.status.idle":"2024-11-06T03:32:24.855683Z","shell.execute_reply.started":"2024-11-06T03:32:24.84255Z","shell.execute_reply":"2024-11-06T03:32:24.85326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.llms import HuggingFaceHub\n\n# set Korean embedding and llm odel\nhf_embeddings = HuggingFaceEmbeddings(model_name=\"jhgan/ko-sroberta-multitask\")\n\nhf_llm = HuggingFaceHub(\n    repo_id=\"skt/kogpt2-base-v2\",\n    model_kwargs={\"task\": \"text-generation\"} ## question-answering tasK X. text-generation\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:11:07.911377Z","iopub.execute_input":"2025-01-23T07:11:07.911819Z","iopub.status.idle":"2025-01-23T07:11:08.672127Z","shell.execute_reply.started":"2025-01-23T07:11:07.911781Z","shell.execute_reply":"2025-01-23T07:11:08.670773Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"prompt = f\"질문: 5*5? \\ n대답:\"\nresponse = gemini_model.genterate_content(prompt)\n\nanswer = response.candidates[0.contant.parts[0].text\nprint(answer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests\nfrom langchain.schema import Document\nfrom bs4 import BeautifulSoup\n\n# for Wikipedia documents (EN, KO)\n\n# from langchain_community.document_loaders import WikipediaLoader\n\n# By default, English documents (https://en.wikipedia.org))\n# def load_Wiki_docs(query):\n#     loader = WikipediaLoader(query=query, load_max_docs=1) # need !pip install wikipedia\n#     documents = loader.load()\n    \n#     text_splitter = RecursiveCharacterTextSplitter(\n#         chunk_size=1000,\n#         chunk_overlap=200\n#     )\n#     splits = text_splitter.split_documents(documents)\n    \n#     return splits\n\n\n# For Korean query, get results from Korean wikipedia website and crawl and parse results\ndef load_Korean_wiki_docs(topic):\n    url = f\"https://ko.wikipedia.org/wiki/{topic}\"\n    \n    response = requests.get(url)\n    response.raise_for_status()  # raise Exception when error occurs\n\n    # HTML parsing and extract body contents\n    soup = BeautifulSoup(response.text, 'html.parser')\n    content = soup.find('div', {'class': 'mw-parser-output'})  # find div including body contents \n    \n    # Extract contents\n    paragraphs = content.find_all('p')\n    text = \"\\n\".join([p.get_text() for p in paragraphs])  # concat all context in <p> tags \n\n\n    \n    # convert to Document object (required for LangChain)\n    documents = [Document(page_content=text, metadata={\"source\": url})]\n    \n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,\n        chunk_overlap=200\n    )\n    splits = text_splitter.split_documents(documents)\n    \n    return splits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:11:13.395908Z","iopub.execute_input":"2025-01-23T07:11:13.396318Z","iopub.status.idle":"2025-01-23T07:11:13.405088Z","shell.execute_reply.started":"2025-01-23T07:11:13.396280Z","shell.execute_reply":"2025-01-23T07:11:13.403543Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"def create_vectorstore(splits): \n    vectorstore = Chroma.from_documents(documents=splits, embedding=hf_embeddings)\n    return vectorstore","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:11:14.113200Z","iopub.execute_input":"2025-01-23T07:11:14.113667Z","iopub.status.idle":"2025-01-23T07:11:14.119262Z","shell.execute_reply.started":"2025-01-23T07:11:14.113630Z","shell.execute_reply":"2025-01-23T07:11:14.117843Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"topic = \"흑백요리사\"\n# Load wikipedia documents for this topic\nsplits = load_Korean_wiki_docs(topic) \n# Create vectorstore with this fetched docs\nvectorstore = create_vectorstore(splits)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:11:16.506735Z","iopub.execute_input":"2025-01-23T07:11:16.507829Z","iopub.status.idle":"2025-01-23T07:11:16.893639Z","shell.execute_reply.started":"2025-01-23T07:11:16.507776Z","shell.execute_reply":"2025-01-23T07:11:16.892626Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"def create_rag_chain(vectorstore):\n    prompt_template = \"\"\"문맥을 참고하여 질문에 정확하고 간결하게 답하십시오.\n    문맥: {context}\n    질문: {question}\n    답변:\"\"\"\n    \n    PROMPT = PromptTemplate(\n        template=prompt_template, input_variables=[\"context\", \"question\"]\n    )\n\n    chain_type_kwargs = {\"prompt\": PROMPT}\n\n    # Make context shorter\n    # def short_context(context, max_length=300):\n    #     return context[:max_length] if len(context) > max_length else context\n    \n    # class ShortContextRetriever(BaseRetriever):\n    #     def __init__(self, retriever):\n    #         super().__init__()\n    #         self._retriever = retriever\n        \n    #     def get_relevant_documents(self, query):\n    #         docs = self._retriever.get_relevant_documents(query)\n    #         for doc in docs:\n    #             doc.page_content = short_context(doc.page_content)\n    #         return docs\n    \n    # retriever = ShortContextRetriever(vectorstore.as_retriever())\n\n    qa_chain = RetrievalQA.from_chain_type(\n        llm=hf_llm,\n        chain_type=\"stuff\",\n        retriever=vectorstore.as_retriever(),\n        chain_type_kwargs=chain_type_kwargs,\n        return_source_documents=True\n    )\n    \n    return qa_chain","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:11:18.449140Z","iopub.execute_input":"2025-01-23T07:11:18.449529Z","iopub.status.idle":"2025-01-23T07:11:18.457345Z","shell.execute_reply.started":"2025-01-23T07:11:18.449497Z","shell.execute_reply":"2025-01-23T07:11:18.456044Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# create langchang RAG chain\nqa_chain = create_rag_chain(vectorstore)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:11:21.018885Z","iopub.execute_input":"2025-01-23T07:11:21.019286Z","iopub.status.idle":"2025-01-23T07:11:21.026008Z","shell.execute_reply.started":"2025-01-23T07:11:21.019249Z","shell.execute_reply":"2025-01-23T07:11:21.024490Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"question = \"심사위원을 누가 맡았어?\"\n\n# result = qa_chain({\"query\": question})\nresult = qa_chain.invoke({\"query\": question})\n\nprint (\"결과:\")\nprint(result[\"result\"])\n\nprint(\"출처:\")\nfor doc in result[\"source_documents\"]:\n    print(doc.page_content)\n    print(\"---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:11:23.069720Z","iopub.execute_input":"2025-01-23T07:11:23.070799Z","iopub.status.idle":"2025-01-23T07:11:23.466183Z","shell.execute_reply.started":"2025-01-23T07:11:23.070747Z","shell.execute_reply":"2025-01-23T07:11:23.464920Z"}},"outputs":[{"name":"stdout","text":"결과:\n문맥을 참고하여 질문에 정확하고 간결하게 답하십시오.\n    문맥: 《흑백요리사: 요리 계급 전쟁》(영어: Culinary Class Wars)은 넷플릭스의 요리 서바이벌 프로그램이다. 방송 직후 세계 여러 나라에서 시청률 1위를 기록했고, 대만인들의 한국 관광 열풍과 한국 음식에 대한 사랑을 불러일으켰다. 유명 레스토랑 셰프 등 100인의 요리사가 출연한다. 심사위원은 백종원과 안성재가 맡았다. 가제는 《무명요리사》였다.[1]\n\n《흑백요리사: 요리 계급 전쟁》(영어: Culinary Class Wars)은 넷플릭스의 요리 서바이벌 프로그램이다. 방송 직후 세계 여러 나라에서 시청률 1위를 기록했고, 대만인들의 한국 관광 열풍과 한국 음식에 대한 사랑을 불러일으켰다. 유명 레스토랑 셰프 등 100인의 요리사가 출연한다. 심사위원은 백종원과 안성재가 맡았다. 가제는 《무명요리사》였다.[1]\n    질문: 심사위원을 누가 맡았어?\n    답변: 《흑백요리사: 요리 계급 전쟁》(영어: Culinary Class Wars)는 넷플릭스의 요리 서바이벌 프로그램이다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원은 백종원과 안성재가 맡았다.\n심사위원\n출처:\n《흑백요리사: 요리 계급 전쟁》(영어: Culinary Class Wars)은 넷플릭스의 요리 서바이벌 프로그램이다. 방송 직후 세계 여러 나라에서 시청률 1위를 기록했고, 대만인들의 한국 관광 열풍과 한국 음식에 대한 사랑을 불러일으켰다. 유명 레스토랑 셰프 등 100인의 요리사가 출연한다. 심사위원은 백종원과 안성재가 맡았다. 가제는 《무명요리사》였다.[1]\n---\n《흑백요리사: 요리 계급 전쟁》(영어: Culinary Class Wars)은 넷플릭스의 요리 서바이벌 프로그램이다. 방송 직후 세계 여러 나라에서 시청률 1위를 기록했고, 대만인들의 한국 관광 열풍과 한국 음식에 대한 사랑을 불러일으켰다. 유명 레스토랑 셰프 등 100인의 요리사가 출연한다. 심사위원은 백종원과 안성재가 맡았다. 가제는 《무명요리사》였다.[1]\n---\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"docs = vectorstore.as_retriever().get_relevant_documents(question)\ndocs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:33:15.322779Z","iopub.execute_input":"2024-11-06T03:33:15.323234Z","iopub.status.idle":"2024-11-06T03:33:15.485525Z","shell.execute_reply.started":"2024-11-06T03:33:15.323191Z","shell.execute_reply":"2024-11-06T03:33:15.484176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"docs = vectorstore.similarity_search(question, k=4)\ndocs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:33:15.487481Z","iopub.execute_input":"2024-11-06T03:33:15.488034Z","iopub.status.idle":"2024-11-06T03:33:15.574314Z","shell.execute_reply.started":"2024-11-06T03:33:15.487979Z","shell.execute_reply":"2024-11-06T03:33:15.572843Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# It seems vectorDB loading from embedding model works fine, but seems llm model does not.\n# Some Korean llm model seems to work fine in text-generation task, but for Question-Ansering task, we might need another approach.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:33:15.580119Z","iopub.execute_input":"2024-11-06T03:33:15.580617Z","iopub.status.idle":"2024-11-06T03:33:15.586739Z","shell.execute_reply.started":"2024-11-06T03:33:15.580573Z","shell.execute_reply":"2024-11-06T03:33:15.585086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Use QA pipeline with vectorstor similarity search","metadata":{}},{"cell_type":"code","source":"# import torch\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\n# Load model and tokenizer\nmodel_name = \"yjgwak/klue-bert-base-finetuned-squard-kor-v1\"\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Set Q_A pipeline\nqa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:09:12.393290Z","iopub.execute_input":"2025-01-23T07:09:12.393960Z","iopub.status.idle":"2025-01-23T07:09:12.740897Z","shell.execute_reply.started":"2025-01-23T07:09:12.393910Z","shell.execute_reply":"2025-01-23T07:09:12.739604Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# Example: define question and context \nquestion = \"오늘 날씨 어때?\"\ncontext = \"오늘의 날씨는 맑고 따뜻한 기온이 유지될 것으로 보입니다.\"\n\n# model chain\nresult = qa_pipeline(question=question, context=context)\n\n# Result\nprint(\"질문:\", question)\nprint(\"답변:\", result['answer'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:09:16.006942Z","iopub.execute_input":"2025-01-23T07:09:16.007357Z","iopub.status.idle":"2025-01-23T07:09:16.118541Z","shell.execute_reply.started":"2025-01-23T07:09:16.007319Z","shell.execute_reply":"2025-01-23T07:09:16.117368Z"}},"outputs":[{"name":"stdout","text":"질문: 오늘 날씨 어때?\n답변: 맑고 따뜻한 기온이\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# search context in VectorStore\ndef retrieve_context(question, vectorstore):\n    docs = vectorstore.similarity_search(question, k=4)\n    if docs:\n        return \" \".join([doc.page_content for doc in docs])\n        # return docs[0].page_content  # return first relevant doc\n    else:\n        return None\n\n# Generate answer based on query and searched context similar to RAG chain\ndef answer_question_with_context(question, vectorstore):\n    context = retrieve_context(question, vectorstore)\n    if context:\n        result = qa_pipeline(question=question, context=context)\n        return result['answer'], context  # return answer and used source doc\n    else:\n        return \"관련 문맥을 찾지 못했습니다.\", None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:09:20.931484Z","iopub.execute_input":"2025-01-23T07:09:20.931927Z","iopub.status.idle":"2025-01-23T07:09:20.939452Z","shell.execute_reply.started":"2025-01-23T07:09:20.931884Z","shell.execute_reply":"2025-01-23T07:09:20.938029Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# Example\nquestion = \"심사위원을 누가 맡았어?\"\n\nanswer, used_context = answer_question_with_context(question, vectorstore)\n\nprint(\"질문:\", question)\nprint(\"답변:\", answer)\nprint(\"사용된 문맥:\", used_context)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:09:23.420313Z","iopub.execute_input":"2025-01-23T07:09:23.420703Z","iopub.status.idle":"2025-01-23T07:09:23.860696Z","shell.execute_reply.started":"2025-01-23T07:09:23.420670Z","shell.execute_reply":"2025-01-23T07:09:23.859401Z"}},"outputs":[{"name":"stdout","text":"질문: 심사위원을 누가 맡았어?\n답변: 백종원과 안성재가\n사용된 문맥: 《흑백요리사: 요리 계급 전쟁》(영어: Culinary Class Wars)은 넷플릭스의 요리 서바이벌 프로그램이다. 방송 직후 세계 여러 나라에서 시청률 1위를 기록했고, 대만인들의 한국 관광 열풍과 한국 음식에 대한 사랑을 불러일으켰다. 유명 레스토랑 셰프 등 100인의 요리사가 출연한다. 심사위원은 백종원과 안성재가 맡았다. 가제는 《무명요리사》였다.[1] 《흑백요리사: 요리 계급 전쟁》(영어: Culinary Class Wars)은 넷플릭스의 요리 서바이벌 프로그램이다. 방송 직후 세계 여러 나라에서 시청률 1위를 기록했고, 대만인들의 한국 관광 열풍과 한국 음식에 대한 사랑을 불러일으켰다. 유명 레스토랑 셰프 등 100인의 요리사가 출연한다. 심사위원은 백종원과 안성재가 맡았다. 가제는 《무명요리사》였다.[1]\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Use Gemini+RAG","metadata":{}},{"cell_type":"code","source":"from IPython.display import display\nimport ipywidgets as widgets\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:50:59.931962Z","iopub.execute_input":"2025-01-23T07:50:59.932374Z","iopub.status.idle":"2025-01-23T07:50:59.938277Z","shell.execute_reply.started":"2025-01-23T07:50:59.932341Z","shell.execute_reply":"2025-01-23T07:50:59.936882Z"}},"outputs":[],"execution_count":99},{"cell_type":"code","source":"# It seems the best and simple and cost-free option when OpenAI api cannot be used.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:33:31.904985Z","iopub.execute_input":"2024-11-06T03:33:31.905918Z","iopub.status.idle":"2024-11-06T03:33:31.912362Z","shell.execute_reply.started":"2024-11-06T03:33:31.905848Z","shell.execute_reply":"2024-11-06T03:33:31.910324Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -q langchain langchain-community langchain_huggingface chromadb google-generativeai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:52:16.165681Z","iopub.execute_input":"2025-01-23T07:52:16.166910Z","iopub.status.idle":"2025-01-23T07:52:29.194456Z","shell.execute_reply.started":"2025-01-23T07:52:16.166830Z","shell.execute_reply":"2025-01-23T07:52:29.192943Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":103},{"cell_type":"code","source":"# pip install google-generativeai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T03:33:52.362965Z","iopub.execute_input":"2024-11-06T03:33:52.363439Z","iopub.status.idle":"2024-11-06T03:33:52.369738Z","shell.execute_reply.started":"2024-11-06T03:33:52.363381Z","shell.execute_reply":"2024-11-06T03:33:52.368235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.schema import Document\nfrom langchain.llms import OpenAI\nimport google.generativeai as genai\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:52:37.332429Z","iopub.execute_input":"2025-01-23T07:52:37.332923Z","iopub.status.idle":"2025-01-23T07:52:37.340175Z","shell.execute_reply.started":"2025-01-23T07:52:37.332881Z","shell.execute_reply":"2025-01-23T07:52:37.338660Z"}},"outputs":[],"execution_count":104},{"cell_type":"code","source":"# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"YOUR-API-KEY\"\ngenai_api_key = \"u r api\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:52:41.518354Z","iopub.execute_input":"2025-01-23T07:52:41.518741Z","iopub.status.idle":"2025-01-23T07:52:41.524022Z","shell.execute_reply.started":"2025-01-23T07:52:41.518708Z","shell.execute_reply":"2025-01-23T07:52:41.522764Z"}},"outputs":[],"execution_count":105},{"cell_type":"code","source":"genai.configure(api_key=genai_api_key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:52:42.444220Z","iopub.execute_input":"2025-01-23T07:52:42.444598Z","iopub.status.idle":"2025-01-23T07:52:42.450302Z","shell.execute_reply.started":"2025-01-23T07:52:42.444567Z","shell.execute_reply":"2025-01-23T07:52:42.449092Z"}},"outputs":[],"execution_count":106},{"cell_type":"code","source":"# 1. Gemini model\ngemini_model = genai.GenerativeModel('gemini-1.5-flash')\n\n# 2. embedding model\nembedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:52:43.891619Z","iopub.execute_input":"2025-01-23T07:52:43.892054Z","iopub.status.idle":"2025-01-23T07:52:47.213327Z","shell.execute_reply.started":"2025-01-23T07:52:43.892016Z","shell.execute_reply":"2025-01-23T07:52:47.211974Z"}},"outputs":[],"execution_count":107},{"cell_type":"code","source":"from langchain.vectorstores import Chroma\n# sample docs\ndocs = [\n    Document(page_content=\"한국어 챗봇은 자연어 처리 기술을 사용하여 사용자와 대화를 나눕니다.\", metadata={\"source\": \"doc1\"}),\n    Document(page_content=\"인공지능을 활용한 챗봇은 여러 산업에서 사용되고 있습니다.\", metadata={\"source\": \"doc2\"}),\n    Document(page_content=\"한국어와 영어를 동시에 지원하는 챗봇이 점점 늘어나고 있습니다.\", metadata={\"source\": \"doc3\"}),\n    Document(page_content=\"챗봇은 고객 서비스를 개선하고 사용자 경험을 향상시키는 데 중요한 역할을 합니다.\", metadata={\"source\": \"doc4\"})\n]\n\n# to avoid collision with previous one\npersist_directory = \"./new_chroma_db\"\n\nvectorstore = Chroma.from_documents(splits, embedding=embedding_model, persist_directory=\"./chroma_db\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:52:47.278144Z","iopub.execute_input":"2025-01-23T07:52:47.278637Z","iopub.status.idle":"2025-01-23T07:52:47.392208Z","shell.execute_reply.started":"2025-01-23T07:52:47.278598Z","shell.execute_reply":"2025-01-23T07:52:47.390976Z"}},"outputs":[],"execution_count":108},{"cell_type":"code","source":"# RAG using prompt\ndef rag_chatbot(question):\n    context_doc = vectorstore.similarity_search(question, k=4)\n    #context = context_doc[0].page_content if context_doc else \"정보를 찾을 수 없습니다.\"\n\n    context = \" \".join([doc.page_content for doc in context_doc])\n    \n    prompt = f\"Context: {context}\\질문: {question}\\짧게 대답해줘 :\"\n    # response = gemini_model(prompt)\n    \n    response = gemini_model.generate_content(prompt)\n    answer = response.candidates[0].content.parts[0].text\n\n    #print(\"출처 문서:\", context)\n    return answer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:52:49.709838Z","iopub.execute_input":"2025-01-23T07:52:49.710280Z","iopub.status.idle":"2025-01-23T07:52:49.718207Z","shell.execute_reply.started":"2025-01-23T07:52:49.710243Z","shell.execute_reply":"2025-01-23T07:52:49.715987Z"}},"outputs":[],"execution_count":109},{"cell_type":"code","source":"# sample question\nquestion = \"어떻게 지내니?\"\nresponse = rag_chatbot(question)\n\nprint(\"질문:\", question)\nprint(\"답변:\", response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:53:05.493890Z","iopub.execute_input":"2025-01-23T07:53:05.494360Z","iopub.status.idle":"2025-01-23T07:53:05.897761Z","shell.execute_reply.started":"2025-01-23T07:53:05.494324Z","shell.execute_reply":"2025-01-23T07:53:05.896240Z"}},"outputs":[{"name":"stdout","text":"질문: 어떻게 지내니?\n답변: 잘 지내요.\n\n","output_type":"stream"}],"execution_count":110},{"cell_type":"code","source":"prompt = f\"질문: 질풍가도에 대해 알려줘? \\ n대답:\"\nresponse = gemini_model.generate_content(prompt)\n\nanswer = response.candidates[0].content.parts[0].text\nprint(answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T07:54:35.097310Z","iopub.execute_input":"2025-01-23T07:54:35.097683Z","iopub.status.idle":"2025-01-23T07:54:37.742990Z","shell.execute_reply.started":"2025-01-23T07:54:35.097651Z","shell.execute_reply":"2025-01-23T07:54:37.741770Z"}},"outputs":[{"name":"stdout","text":"질풍가도(疾風歌道)는 여러 가지 의미로 해석될 수 있는 표현입니다. 문맥에 따라 다음과 같은 의미를 가질 수 있습니다.\n\n* **일반적인 의미:** 맹렬한 기세로 앞으로 나아가는 모습, 급속한 발전이나 성장, 거침없는 행보를 묘사하는 관용적인 표현입니다.  빠르고 역동적인 움직임과 압도적인 기세를 강조합니다.  마치 폭풍처럼 거세게 몰아치는 모습을 연상시키죠.\n\n* **문학/예술 작품 제목으로:** 특정 소설, 영화, 음악 등의 제목으로 사용될 수 있으며,  그 작품의 주제나 내용을 암시합니다.  작품의 내용에 따라 질풍가도라는 제목이 가진 의미는 달라질 수 있습니다.  예를 들어, 주인공의 격렬한 삶이나 급격한 변화를 나타낼 수 있습니다.\n\n* **개인의 삶의 태도로:**  자신의 삶을 적극적으로 개척하고, 목표를 향해 끊임없이 도전하는 정신을 나타낼 수 있습니다.  어려움에도 굴하지 않고 끊임없이 전진하는 강인한 의지를 보여줍니다.\n\n\n질풍가도에 대한 더 자세한 정보를 얻으시려면, 어떤 맥락에서 이 단어를 접하셨는지 알려주시면 더 정확한 답변을 드릴 수 있습니다.  예를 들어, 특정 작품의 제목으로 사용된 경우 작품의 제목을 알려주시면 좋습니다.\n\n","output_type":"stream"}],"execution_count":112},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}